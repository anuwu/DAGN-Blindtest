{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"blindtest.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":["1VTO6oYCmzW6","UhCtuSEdByT4"],"toc_visible":true,"machine_shape":"hm"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/anwesh0304/DAGN-Blindtest/blob/master/blindtest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"1VTO6oYCmzW6","colab_type":"text"},"source":["#**Welcome to the SDSS Double Nuclei Detection Pipeline**\n","\n","*Author : Anwesh Bhattacharya, E-mail : f2016590@pilani.bits-pilani.ac.in*\n","\n","Welcome to the pipeline detection software. \n","\n","The pipeline basically needs a .csv file containing objIDs of galaxies to run. It works in three phases -\n","1. Download phase - All the FITS files corresponding to the objIDs are downloaded and the cutout is performed to prepare for the next phase.\n","2. Process phase - This is the heart of the pipeline with the image processing algorithms. It outputs the result - single/double/neither - for each galaxy.\n","3. Segregation phase - Many files are generated in the above two phases of the pipeline. These files are then grouped neatly into folders for the ease of manual inspection.\n","\n","In  order to query your own .csv file on CasJobs and contribute to the group effort of finding dual AGN candidates, please send an email to **dagn2020iia@gmail.com** with your SciServer username, and I shall add you to the **AstrIRG_DAGN** group.\n","\n","For more information, check out the readme at the GitHub repository https://github.com/anwesh0304/DAGN-Blindtest\n"]},{"cell_type":"markdown","metadata":{"id":"nKUy5QLuziq7","colab_type":"text"},"source":["#**Preparations**\n","\n","Ensure that you complete the following steps before running the notebook -\n","\n","\n","1. Extract the zip file containing the source code files and place them in a directory in your google drive\n","2. Create another directory in your google drive and place the .csv that contains the *objIDs*, in that directory.\n","\n","Prevent Google Colab from disconnecting by doing the following -\n","\n","```\n","function ClickConnect(){\n","console.log(\"Working\"); \n","document.querySelector(\"colab-toolbar-button#connect\").click() \n","}\n","setInterval(ClickConnect,60000)\n","```\n","\n","1. Press Ctrl + Shift + I to open the console.\n","2. Paste the code in the consle and press enter.\n","3. Close the console\n","\n","This will click the running window every 60 seconds and prevent Google Colab from disconnecting automatically.\n","\n","**You're done! Now run the following cells in order one at a time**"]},{"cell_type":"markdown","metadata":{"id":"Llx2afIA0hub","colab_type":"text"},"source":["## CLI\n","\n","Tinker with Google's Ubuntu kernel if needed"]},{"cell_type":"code","metadata":{"id":"vMTobati0yVf","colab_type":"code","colab":{}},"source":["! git status"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NCqsukjYU2Se","colab_type":"text"},"source":["##Mount Google Drive\n","\n","The first time this is done, you'll need to give permission to access the drive. \n"]},{"cell_type":"code","metadata":{"id":"ksBL6ZUoNg2J","colab_type":"code","colab":{}},"source":["# On master branch.\n","\n","from google.colab import drive\n","drive_root = \"/content/drive\"\n","drive.mount(drive_root, force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m8PY866kMGoC","colab_type":"text"},"source":["If entered correctly you should see the following in the console -\n","```\n","Mounted at /content/drive\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"bm3-MntNVAOU","colab_type":"text"},"source":["##Input the Directory where source files are presesnt\n","\n","Enter the relative path (relative to your google drive) of the source files.\n","\n","The root directory of your google drive is  */My Drive'*\n","\n","If the source code is in *'/My Drive/Work/Source'*, then please enter *'Work/Source'* in the form (**without quotes**).\n","\n"]},{"cell_type":"code","metadata":{"id":"Ju8aZvu8NYc_","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import os\n","import sys\n","import shutil\n","\n","reldir = 'DAGN/DAGN-Blindtest' #@param {type:\"string\"}\n","\n","my_drive = drive_root + \"/My Drive\"\n","dirr = my_drive + \"/\" + reldir\n","\n","try :\n","  os.chdir(dirr)\n","  source_list = [\"cutout.py\",\n","                 \"dfs.py\",\n","                 \"env_level_peak_search.py\",\n","                 \"grad_asc.py\",\n","                 \"notify.pyc\",\n","                 \"peak_util.py\",\n","                 \"sdss_scrape.py\"]\n","\n","\n","  for src in source_list :\n","    shutil.copy (dirr + \"/\" + src, \"/content/\" +src)\n","\n","  print (\"Source directory entered.\")\n","except :\n","  print (\"Error in loading source.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eHLtPGRQMBom","colab_type":"text"},"source":["If entered correctly, you should see the following in the console -\n","```\n","Source directory entered.\n","```\n","\n","Else, you shall see -\n","```\n","Incorrect source directory.\n","```"]},{"cell_type":"markdown","metadata":{"id":"W7hP7Q-DVXsZ","colab_type":"text"},"source":["##Input the directory of CSV file and the CSV filename\n","\n","\n","\n","1. Enter the relative path of the directory where the .csv file is placed. Use the same format is mentioned in the previous cell. \n","2. Enter the name of the CSV file. If the name of your .csv file is *'test.csv'* in the CSV directory, then enter 'test' (**without quotes**)\n","\n","\n","Running this cell will also create a /Data folder in the directory where the final outputs of the pipeline will be stored."]},{"cell_type":"code","metadata":{"id":"rnH3lqJANYdE","colab_type":"code","colab":{}},"source":["csv_path = 'DAGN/Gimeno' #@param {type:\"string\"}\n","csv_abs_path = my_drive + \"/\" + csv_path \n","\n","csv_filename = 'gimeno' #@param {type:\"string\"}\n","\n","try :\n","  if not os.path.exists(csv_abs_path + \"/Data\") :\n","    os.mkdir (csv_abs_path + \"/Data\")\n","  \n","  data = pd.read_csv (csv_abs_path + \"/\" + csv_filename + \".csv\", usecols=['objID'])\n","  print (data)\n","except :\n","  print (\"Incorrect CSV directory or filename\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0CfNAigRMN7u","colab_type":"text"},"source":["If the correct path and name is entered, you should see the csv data in the following fashion - \n","\n","```\n","                   objID\n","0    1237663716018356335\n","1    1237660024527913378\n","2    1237666407919453060\n","3    1237663716555162604\n","4    1237660239777694279\n","..                   ...\n","995  1237663782599722190\n","996  1237663239272530594\n","997  1237657069548733287\n","998  1237663782591922283\n","999  1237660240313713520\n","\n","[1000 rows x 1 columns]\n","```\n","\n","Else you'll see the following in the console -\n","\n","```\n","Incorrect CSV directory or filename\n","```"]},{"cell_type":"markdown","metadata":{"id":"h_R-3GmLXsYG","colab_type":"text"},"source":["##Type your email address for receiving notifications\n","\n","For a list size of 1000 galaxies, the downloading phase takes approximately 100 minutes. The processing phase would take another 40 minutes. Hence, you might want to receive regular e-mail notifications about the status of the pipeline.\n","\n","To receive notifications, fill the form with a valid e-mail id. If you do not wish to receive notifications, please ensure the form field is **empty**.\n","\n","In case you subscribe to the email notifications, you'll receive emails from dagn2020iia@gmail.com\n","\n","*Fun fact - The profile icon of this email id is the well known double nuclei galaxy NGC 3758*\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"BfPSPkkOXgVQ","colab_type":"code","colab":{}},"source":["import notify as notif\n","\n","receiver = '' #@param {type:\"string\"}\n","yes_mail = True\n","if receiver == '' :\n","  yes_mail = False\n","\n","if yes_mail :\n","  try :\n","    notif.send_mail (receiver, \"Welcome to the SDSS DAGN Pipeline.\", yes_mail)\n","    print (\"Email : \" + receiver)\n","  except :\n","    print (\"Please enter a valid mail\")\n","else :\n","  print (\"No email\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hjlpy6BKMbYL","colab_type":"text"},"source":["If you've entered the email as *hello@gmail.com*, you'll see the following in the console -\n","```\n","Email : hello@gmail.com\n","```\n","\n","Else, you shall see -\n","```\n","No email\n","```\n","\n","In case you've entered an invalid email, you'll get a warning -\n","\n","```\n","Please enter a valid e-mail\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"N4uJiflYuZyx","colab_type":"text"},"source":["##Download Notification Interval\n","\n","Specify the **percentage interval** at which you would like to keep tabs on the downloading phase. For a rough idea, downloading 50 files takes around 5 minutes. Hence, if you wish to obtain notifications for every 10% progress of the download phase (*and subsequently receive emails every 10 minutes*), then enter '10' in the form field.\n","\n","Please enter one of (1, 2, 4, 5, 10, 20, 25, 50) in the field **regardless** of whether you've entered the e-mail field. This is needed for the purpose of the dumpfile. \n","\n"]},{"cell_type":"code","metadata":{"id":"JesAJh-AqsZJ","colab_type":"code","colab":{}},"source":["download_list_step =  1#@param {type:\"number\"}\n","\n","if download_list_step not in [1,2,4,5,10,20,25,50] :\n","  print (\"Invalid step, please enter again.\")\n","else :\n","  print (\"Downloading step : \" + str(download_list_step) + \"%\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lyK8R7gAMtda","colab_type":"text"},"source":["If you've entered a valid interval (*such as 25*), you'll see the following in the console -\n","```\n","Downloading step : 25%\n","```\n","\n","Else, you shall see -\n","\n","\n","```\n","Invalid step, please enter again.\n","```"]},{"cell_type":"markdown","metadata":{"id":"UuU0OVFduf5i","colab_type":"text"},"source":["##Processing Notification Interval\n","\n","Same format as the previous cell, applied to the processing phase. \n","\n","Kindly enter one of (1, 2, 4, 5, 10, 20, 25, 50) in the form field **regardless** of e-mail notifications.\n","\n"]},{"cell_type":"code","metadata":{"id":"zUOGyl4Cqs9P","colab_type":"code","colab":{}},"source":["process_list_step =  1#@param {type:\"number\"}\n","\n","if process_list_step not in [1,2,4,5,10,20,25,50] :\n","  print (\"Invalid step, please enter again.\")\n","else :\n","  print (\"Processing step : \" + str(process_list_step) + \"%\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5tuHmCq0M6mW","colab_type":"text"},"source":["If you've entered a valid interval (*such as 25*), you'll see the following in the console -\n","```\n","Processing step : 25%\n","```\n","\n","Else, you shall see -\n","\n","\n","```\n","Invalid step, please enter again.\n","```"]},{"cell_type":"markdown","metadata":{"id":"0WByU_v2Vvca","colab_type":"text"},"source":["##Initialisation code before running pipeline\n","\n","Please execute this cell **only once**.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jHGXlBcvRT70","cellView":"code","colab":{}},"source":["import sys\n","import sdss_scrape as scrap\n","import cutout as ct\n","import env_level_peak_search as envpeak\n","\n","import bs4\n","import requests\n","import notify as notif\n","import urllib\n","import bz2\n","import numpy as np\n","import importlib\n","\n","# Increasing depth of recursion to allow graph-searching\n","sys.setrecursionlimit(sys.getrecursionlimit()*30)\n","\n","# Link for scraping objects\n","link = \"http://skyserver.sdss.org/dr14/en/tools/explore/summary.aspx?objid=\" \n","\n","# List of 'hyper-parameters'\n","cutout_radius = 40\t\t# Size of cut-out in arcseconds\n","sigma_x = 7\t\t# Smoothing parameter\n","sigma_y = 7\t\t# Smoothing parameter\n","levels = 20\t\t# levels of contour\n","background_ratio = 0\t\t# if it is zero, use env_level value else use (background_ratio * levels) for defining envelope\n","env_level = 7 \t\t# lower contour level that defines the environment\n","iters = 500\t\t\t# number of iterations to \n","neigh_tol = 3\t\t# neighborhood distance tolerance\t\n","radius = 30\t\t\t# search radius\n","thicc = True  # Thick marking of peaks\n","\n","ct.cutout_test ()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6-8iMkZXM_46","colab_type":"text"},"source":["If the initialisation succeeds, you'll see the output as -\n","\n","```\n","Initialisation complete!\n","```"]},{"cell_type":"markdown","metadata":{"id":"sfInQ14w2BXU","colab_type":"text"},"source":["#**Pipeline**\n","\n","Ensure all the previous cells have been run and they are giving the desired output. \n","\n","If you wish, you can skip directly to the **segregation phase** and run all the cells at once. Otherwise, you can run the cells one at a time."]},{"cell_type":"markdown","metadata":{"id":"_ACryKCXm9pZ","colab_type":"text"},"source":["##Download Phase\n","\n","All the FITS files are downloaded in this phase. Run the code cell below.\n","\n","At the end of running the cell successfully, a dumpfile named *'test_download.csv'* is placed in the CSV directory."]},{"cell_type":"markdown","metadata":{"id":"EozYxyJ_Nw55","colab_type":"text"},"source":["### Download Crashes\n","\n","In case Colab crashes in the middle of the download phase, the latest multiple of the percentage interval is saved. For instance if the following were to occur -\n","\n","\n","1. If you set the interval to 10.\n","2. Your csv file is named 'test.csv'\n","3. Colab crashes at 63% completion.\n","\n","Then, a file named *'test_download_60.csv'* will be dumped in the same directory as objID csv file.\n","\n","If this is to happen, run the cell again, and the cell will sift through the already downloaded FITS files very quickly. The way the source code is written, it does not use the dumpfile on a re-run. It's simply put for user's convenience, so that he/she can manually check the progress.\n","\n","**If at any point you are forced to restart the runtime environment, please run all cells from the beginning**"]},{"cell_type":"code","metadata":{"id":"eLb1RA7AEJWu","colab_type":"code","cellView":"code","colab":{}},"source":["# Reloading imported files to update any change\n","ct = importlib.reload (ct)\n","scrap = importlib.reload (scrap)\n","envpeak = importlib.reload (envpeak)\n","notif = importlib.reload (notif)\n","\n","# Creating download_percent_list based on download notification interval\n","download_percent_list = []\n","for per in range (download_list_step, 100, download_list_step) :\n","  download_percent_list.append (per)\n","\n","# Create download logfile\n","download_file = open(csv_abs_path + \"/\" + csv_filename + \"_download.csv\", \"w\")\n","download_file.write (\"objID,status\\n\")\n","print (\"Starting Downloading \" + csv_filename)\n","\n","# Email notification on downloading start\n","notif.send_mail(receiver, \"Started Downloading \" + csv_filename, yes_mail)\n","\n","i = 0\n","for row in data['objID'] :\n","  try :\n","    i = i + 1\n","    download_file.write (str(row))\n","\n","    for per in download_percent_list :\n","      if i == int(np.ceil(per/100.0 * len(data['objID']))) :\n","        # Email notification on download interval\n","        notif.send_mail (receiver, csv_filename + \" Downloading \" + str(per) + \"%\", yes_mail)\n","\n","        # Closing master download logfile and dumping at percentage interval\n","        download_file.close ()\n","        shutil.copyfile (csv_abs_path + \"/\" + csv_filename + \"_download.csv\", \n","                         csv_abs_path + \"/\" + csv_filename + \"_download_\" + str(per) + \".csv\")\n","        \n","        # Deleting old logfile\n","        if not ((per - download_list_step) == 0) :\n","          os.remove (csv_abs_path + \"/\" + csv_filename + \"_download_\" + str(per-download_list_step) + \".csv\")\n","\n","        # Re-opening master download logfile\n","        download_file = open (csv_abs_path + \"/\" + csv_filename + \"_download.csv\", \"a\")\n","\n","    \n","    \n","    if not os.path.exists (csv_abs_path + \"/Data/\" + str(row) + \"_cut.fits\") :\n","      obj_link = link + str(row)\n","      res = requests.get(obj_link)\n","\n","      # Scraping object page. The two return values of the function are null\n","      # if the object id is inalid\n","      fits_repo , sexa_cood = scrap.obj_link_FITS_repository_link_sexa_cood (obj_link)\n","      if ((fits_repo, sexa_cood) == (None, None)) :\n","        # Output invalid\n","        print (str(i) + \". \" + str(row) + \",invalidObject\")\n","        download_file.write (\",invalidObject\\n\")\n","        continue\n","\n","      # Downloading FITS file bz2 and extracting it\n","      download_link = scrap.FITS_repository_rband_download_link (fits_repo)\n","      scrap.download_extract (download_link, str(row), csv_abs_path)\n","\n","      # Performing cutout\n","      ct.cutout_fits (str(row), sexa_cood, radius, csv_abs_path)\n","\n","    # Output the download result of object\n","    print (str(i) + \". \" + str(row) + \",success\")\n","    download_file.write (\",success\\n\")\n","\n","  except Exception as e :\n","    # Output any exception\n","    print (str(i) + \". \" + str(row) + \" \" + str(e))\n","    download_file.write (\", \" + str(e))\n","\n","# Closing master download logfile\n","download_file.close ()\n","\n","# Removing latest dumpfile\n","os.remove (csv_abs_path + \"/\" + csv_filename + \"_download_\" + str(100-download_list_step) + \".csv\")\n","\n","print (csv_filename + \" Download Completed. Processing \" + csv_filename + \".\")\n","\n","# Email notificaiton on completion of download\n","notif.send_mail (receiver, csv_filename + \" Download Completed. Processing \" + csv_filename + \".\", yes_mail)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-c5ABseiNn4C","colab_type":"text"},"source":["Expected output for *'test.csv'* -\n","\n","```\n","Starting Downloading test\n","1. 1237663785284273004,success\n","2. 1237657070092157995,success\n","3. 1237666300021310385,success\n","4. 1237678437019157791,success\n","5. 1237660240312861658,success\n","6. 1237657071696740894,success\n","7. 1237663783674840359,success\n","8. 1237663784214463461,success\n",".\n",".\n",".\n","1000. 1237660241387586360, success\n","test Downloading Completed. Processing test.\n","```"]},{"cell_type":"markdown","metadata":{"id":"7GnNXiPAEJXb","colab_type":"text"},"source":["##Processing Stage\n","\n","After the download phase has completed, the FITS files are processed and outputs are generated. Run the code cell below.\n","\n","At the end of running the cell, a dumpfile named *'test_output.csv'* is placed in the CSV directory.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bFMWE8pJOv-K","colab_type":"text"},"source":["### Processing Crashes\n","\n","If the notebook crashes in the middle of this phase, the dumpfile is used to quickly recover to the last savepoint. The only difference is that the intermediate dump file will be named *'test_output_60.csv'* \n","\n","**If it appears that the dumpfile is causing interference with the working of the notebook, please delete it manually.**\n","\n","**If at any point you are forced to restart the runtime environment, please run all cells from the beginning**"]},{"cell_type":"code","metadata":{"id":"PtpcdVXBD5hu","colab_type":"code","cellView":"code","colab":{}},"source":["# Creating master processing logfile\n","done_file = open(csv_abs_path + \"/\" + csv_filename + \"_output.csv\",\"w\")\n","done_file.write (\"objID,output\\n\")\n","\n","# Creating logfile of double detections\n","double_file = open (csv_abs_path + \"/\" + csv_filename + \"_double.csv\", \"w\")\n","double_file.write (\"objID\\n\")\n","\n","# Creating process_percent_list based on processing notification interval\n","dump_found = False\n","process_percent_list = []\n","for per in range(process_list_step, 100, process_list_step) :\n","  process_percent_list.append(per)\n","\n","  # Searching for existing dumpfile\n","  if os.path.exists (csv_abs_path + \"/\" + csv_filename + \"_output_\" + str(per) + \".csv\") :\n","    dump_pd = pd.read_csv (csv_abs_path + \"/\" + csv_filename + \"_output_\" + str(per) + \".csv\",\n","                           usecols=['objID' , 'output'], sep=\",\")\n","    dump_found = True\n","\n","double_count = 0 \n","\n","if dump_found : \n","  # Output the data already in dumpfile\n","  for ind in dump_pd.index :\n","    print (str(ind+1) +\". \" + str(dump_pd['objID'][ind]) + \",\" + str(dump_pd['output'][ind]))\n","    done_file.write (str(dump_pd['objID'][ind]) + \",\" + str(dump_pd['output'][ind]) + \"\\n\")\n","    \n","    # Increase double count, and add to '_double.csv' output file\n","    if str(dump_pd['output'][ind]) == 'double' :\n","      double_file.write (str(dump_pd['objID'][ind]) + \"\\n\")\n","      double_count = double_count + 1\n","\n","  start = dump_pd.index.stop\n","else :\n","  start = 0\n","\n","# Start row to read the .csv file. Depends on existence of dumpfile\n","i = start\n","\n","for row in data['objID'].values[start:] :\n","    try :\n","      i = i + 1\n","\n","      for per in process_percent_list :\n","        if i == int(np.ceil(per/100.0 * len(data['objID']))) :\n","          # Email notification on processing interval\n","          notif.send_mail (receiver, csv_filename + \" Processing \" + str(per) + \"%\", yes_mail)\n","\n","          # Closing master processing logfile and dumping at percentage interval\n","          done_file.close ()\n","          shutil.copyfile (csv_abs_path + \"/\" + csv_filename + \"_output.csv\" , \n","                           csv_abs_path + \"/\" + csv_filename + \"_output_\" + str(per) + \".csv\")\n","          \n","          # Deleting old logfile\n","          if not ((per - process_list_step) == 0) :\n","            if os.path.exists (csv_abs_path + \"/\" + csv_filename + \"_output_\"+ str(per-process_list_step) + \".csv\") :\n","              os.remove (csv_abs_path + \"/\" + csv_filename + \"_output_\"+ str(per-process_list_step) + \".csv\")\n","\n","          # Re-oepning master processing logfile\n","          done_file = open (csv_abs_path + \"/\" + csv_filename + \"_output.csv\", \"a\")\n","\n","      \n","      # Loading cutout and extracting pngs, smoothing, and generating contour\n","      Z = ct.cutout_fits2 (str(row) + \"_cut\", csv_abs_path)\n","      ct.export_png_wrapper (str(row) , Z , csv_abs_path, False)\n","      ct.smooth_png_wrapper (str(row) , sigma_x , sigma_y , csv_abs_path, False)\n","      ct.smooth_contour_wrapper (str(row) , Z , levels , csv_abs_path, False)\n","          \n","      # Finding peak\n","      env_is_only_og = envpeak.env_level_peak_plot (str(row) , background_ratio, levels, \n","                                                    env_level ,iters , neigh_tol , thicc, \n","                                                    done_file, csv_abs_path, False)\n","            \n","      msg = str(row)\n","      if env_is_only_og == 'Done' : # Already processed\n","        if os.path.exists (csv_abs_path + \"/Data/\" + str(row) + \"_og_size_env_peaks_only.png\") :\n","          msg = msg + \",single\"\n","        elif os.path.exists (csv_abs_path + \"/Data/\" + str(row) + \"_og_size_env_peaks_top_pair.png\") :\n","          msg = msg + \",double\"\n","          double_file.write (str(row) + \"\\n\")\n","          double_count = double_count + 1\n","        elif os.path.exists (csv_abs_path + \"/Data/\" + str(row) + \"_og_size_env_peaks_none.png\") :\n","          msg = msg + \",noPeak\"\n","      else :\n","        # Classifying the peak\n","        if env_is_only_og == 'Single' :\n","          msg = msg + \",single\"\n","        elif env_is_only_og == 'Double' :\n","          msg = msg + \",double\"\n","          double_file.write (str(row) + \"\\n\")\n","          double_count = double_count + 1\n","        elif env_is_only_og == \"NoPeak\" :\n","          msg = msg + \",noPeak\"\n","        elif env_is_only_og == 'NoNoise' :\n","          msg = msg + \",noNoise\"\n","        elif env_is_only_og == 'Failed' :\n","          msg = msg + \",failed\"\n","        else :\n","          msg = msg + \",unknown condition\"\n","\n","      # Output the result of a row\n","      print (str(i) + \". \" + msg)\n","      done_file.write (msg + \"\\n\")\n","    except Exception as e :\n","      # Output any error\n","      print (str(i) + \". \" + str(row) + \",\" + str(e))\n","      done_file.write (str(row) + \",\" + str(e) + \"\\n\")\n","\n","\n","print (csv_filename + \" Processing Completed. \" + str(double_count) + \n","       \" Double Detected out of \" + str(len(data['objID']))  + \n","       \". Starting Segregation of \" + csv_filename + \".\")\n","\n","# Closing master processing logfile\n","done_file.close()\n","\n","# Closing double logfile\n","double_file.close ()\n","\n","# Removing latest dumpfile\n","os.remove (csv_abs_path + \"/\" + csv_filename + \"_output_\" + str(100-process_list_step) + \".csv\")\n","\n","# Email notificaiton on completion of processing\n","notif.send_mail(receiver, csv_filename + \" Processing Completed. \" + str(double_count) + \n","                \" Double Detected out of \" + str(len(data['objID'])) + \n","                \". Starting Segregation of \" + csv_filename + \".\", yes_mail)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oCEbm1e6Ott2","colab_type":"text"},"source":["\n","Expected output for *'test.csv'* -\n","```\n","1. 1237663785284273004,single\n","2. 1237657070092157995,single\n","3. 1237666300021310385,noPeak\n","4. 1237678437019157791,noNoise\n","5. 1237660240312861658,double\n","6. 1237657071696740894,single\n","7. 1237663783674840359,noPeak\n","8. 1237663784214463461,noPeak\n",".\n",".\n",".\n","1000. 1237660241387586360,single\n","test Processing Completed. 12 Double Detected out of 1000. Starting Segregation of test.\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"v3zP1gYzEOca","colab_type":"text"},"source":["##Segregation Stage\n","\n","After the processing phase, the CSV directory is neatly arranged in the following structure -\n","\n","\n","\n","> /CSV Directory/ \\\n",">> test.csv \\\n",">> test_download.csv \\\n",">> test_double.csv \\\n",">> test_output.csv \\\n",">> /Data/ \\\n",">>> /FITS \\\n",">>> /Cutouts \\\n",">>> /Smooth Cutouts \\\n",">>> /Contours \\\n",">>> /Peaks/\n",">>>> /Single/ \\\n",">>>> /Double/ \\\n",">>>> /NoPeak/ \\\n","\n","\n","Run the code cell below.\n"]},{"cell_type":"code","metadata":{"id":"EJr-hfatXVyY","colab_type":"code","cellView":"code","colab":{}},"source":["# Creating directories based on heirarchy as described in above text-cell\n","\n","if not os.path.exists (csv_abs_path + \"/Data/FITS\") :\n","  os.mkdir (csv_abs_path + \"/Data/FITS\")\n","if not os.path.exists (csv_abs_path + \"/Data/Cutouts\") :\n","  os.mkdir (csv_abs_path + \"/Data/Cutouts\")\n","if not os.path.exists (csv_abs_path + \"/Data/Smooth Cutouts\") :\n","  os.mkdir (csv_abs_path + \"/Data/Smooth Cutouts\")\n","if not os.path.exists (csv_abs_path + \"/Data/Contours\") :\n","  os.mkdir (csv_abs_path + \"/Data/Contours\")\n","\n","if not os.path.exists (csv_abs_path + \"/Data/Peaks\") :\n","  os.mkdir (csv_abs_path + \"/Data/Peaks\")\n","if not os.path.exists (csv_abs_path + \"/Data/Peaks/Single\") :\n","  os.mkdir (csv_abs_path + \"/Data/Peaks/Single\")\n","if not os.path.exists (csv_abs_path + \"/Data/Peaks/Double\") :\n","  os.mkdir (csv_abs_path + \"/Data/Peaks/Double\")\n","if not os.path.exists (csv_abs_path + \"/Data/Peaks/NoPeak\") :\n","  os.mkdir (csv_abs_path + \"/Data/Peaks/NoPeak\")\n","\n","\n","# List of file appends\n","appends = [\"_cut.fits\",\n","           \"_og_size_cutout.png\", \n","           \"_og_size_cutout_smooth.png\", \n","           \"_og_size_contour.png\",\n","           \"_og_size_env_peaks_only.png\",\n","           \"_og_size_env_peaks_top_pair.png\",\n","           \"_og_size_env_peaks_none.png\"]\n","\n","# Directories corresponding to file appends\n","append_dir_dict = {\"_cut.fits\" : \"FITS/\",\n","                   \"_og_size_cutout.png\" : \"Cutouts/\", \n","                   \"_og_size_cutout_smooth.png\" : \"Smooth Cutouts/\", \n","                   \"_og_size_contour.png\" : \"Contours/\",\n","                   \"_og_size_env_peaks_only.png\" : \"Peaks/Single/\",\n","                   \"_og_size_env_peaks_top_pair.png\" : \"Peaks/Double/\",\n","                   \"_og_size_env_peaks_none.png\" : \"Peaks/NoPeak/\"}\n","\n","# Organizing files\n","for row in data['objID'] :\n","  for app in appends :\n","    if os.path.exists (csv_abs_path + \"/Data/\" + str(row) + app) :\n","      shutil.move (csv_abs_path + \"/Data/\" + str(row) + app, csv_abs_path + \"/Data/\" + append_dir_dict[app] + str(row) + app)\n","      #os.remove (csv_abs_path + \"/Data/\" + str(row) + app)\n","\n","print (\"Segregation of \" + csv_filename + \" completed.\")\n","\n","# E-mail notification of completion of segregation\n","notif.send_mail(receiver, \"Segregation of \" + csv_filename + \" completed.\", yes_mail)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gysnFKKmPrMw","colab_type":"text"},"source":["Expected output for *'test.csv'* -\n","```\n","Segregation of test completed.\n","```"]},{"cell_type":"markdown","metadata":{"id":"UhCtuSEdByT4","colab_type":"text"},"source":["#**Conclusion**\n","\n","You may now -\n","1. Close this notebook\n","2. Download the output by entering the CSV directory via your Google Drive.\n","3. Re-run the preparotory steps of the pipeline again for a different CSV file, changing the form fields accordingly.\n","\n","The final file of interest is the *'test_output.csv'* file. It contains the label corresponding to each galaxy ID. The galaxies which are potentially DAGNs are labelled as 'double'.\n","\n","The /Data directory in the CSV directory has the files segregated into folders. The naming of the files is self-explanatory.\n","\n"]}]}